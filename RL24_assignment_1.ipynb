{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "pYs6LMEbNqoQ",
        "1kuCtcDkHQ9N"
      ]
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "metadata": {
        "id": "pYs6LMEbNqoQ"
      },
      "cell_type": "markdown",
      "source": [
        "# RL homework 1\n",
        "\n",
        "Submitted by: (name + id), (name + id)\n",
        "\n",
        "**Due date: 14 February 2024, 23:55am**"
      ]
    },
    {
      "metadata": {
        "id": "6Sns0IKYNtsA"
      },
      "cell_type": "markdown",
      "source": [
        "## How to submit\n",
        "\n",
        "Submissions in pairs only.\n",
        "\n",
        "You should save a copy of the notebook to Google Drive and open it with Google Colab. Then answer all the questions inside the notebook, at the designated cells. Only the notebook will be submitted in moodle (in `.ipynb` format).\n",
        "\n",
        "**Important:** All plots, results and outputs should be included in the notebook as the cells' outputs (run all cells and do not clear the output).\n",
        "    \n",
        "When you have completed the exercises and everything has finished running, click on 'File' in the menu-bar and then 'Download .ipynb'.\n",
        "\n",
        "Then submit the downloaded '.ipynb' file to Moodle."
      ]
    },
    {
      "metadata": {
        "id": "9v_SYckYfv5G"
      },
      "cell_type": "markdown",
      "source": [
        "## Context\n",
        "\n",
        "In this assignment, we will take a first look at learning decisions from data.  \n",
        "\n",
        "## Background reading\n",
        "\n",
        "* Sutton and Barto (2018), Chapters 3 - 6"
      ]
    },
    {
      "metadata": {
        "id": "rNuohp44N00i"
      },
      "cell_type": "markdown",
      "source": [
        "# The Assignment\n",
        "\n",
        "### Objectives\n",
        "\n",
        "You will use Python to implement several reinforcement learning algorithms.\n",
        "\n",
        "You will then run these algorithms on a few problems, to understand their properties."
      ]
    },
    {
      "metadata": {
        "id": "ztQEQvnKh2t6"
      },
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ]
    },
    {
      "metadata": {
        "id": "qB0tQ4aiAaIu"
      },
      "cell_type": "markdown",
      "source": [
        "### Import Useful Libraries"
      ]
    },
    {
      "metadata": {
        "id": "YzYtxi8Wh5SJ"
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from collections import namedtuple"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6NDhSYfSDcCC"
      },
      "cell_type": "markdown",
      "source": [
        "### Set options"
      ]
    },
    {
      "metadata": {
        "id": "Ps5OnkPmDbMX"
      },
      "cell_type": "code",
      "source": [
        "np.set_printoptions(precision=3, suppress=1)\n",
        "plt.style.use('seaborn-notebook')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ALrRR76eAd6u"
      },
      "cell_type": "markdown",
      "source": [
        "### A grid world"
      ]
    },
    {
      "metadata": {
        "id": "YP97bVN3NuG8"
      },
      "cell_type": "code",
      "source": [
        "class Grid(object):\n",
        "\n",
        "  def __init__(self, noisy=False):\n",
        "    # -1: wall\n",
        "    # 0: empty, episode continues\n",
        "    # other: number indicates reward, episode will terminate\n",
        "    self._layout = np.array([\n",
        "      [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1],\n",
        "      [-1,  0,  0,  0,  0,  0,  0,  0,  0,  0, -1],\n",
        "      [-1,  0,  0,  0, -1, -1, -1,  0,  0,  0, -1],\n",
        "      [-1,  0,  0,  0, -1, -1, -1,  0, 10,  0, -1],\n",
        "      [-1,  0,  0,  0, -1, -1, -1,  0,  0,  0, -1],\n",
        "      [-1,  0,  0,  0,  0,  0,  0,  0,  0,  0, -1],\n",
        "      [-1,  0,  0,  0,  0,  0,  0,  0,  0,  0, -1],\n",
        "      [-1,  0,  0,  0,  0,  0,  0,  0,  0,  0, -1],\n",
        "      [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]\n",
        "    ])\n",
        "    self._start_state = (2, 2)\n",
        "    self._state = self._start_state\n",
        "    self._number_of_states = np.prod(np.shape(self._layout))\n",
        "    self._noisy = noisy\n",
        "\n",
        "  @property\n",
        "  def number_of_states(self):\n",
        "      return self._number_of_states\n",
        "\n",
        "  def plot_grid(self):\n",
        "    plt.figure(figsize=(4, 4))\n",
        "    plt.imshow(self._layout > -1, interpolation=\"nearest\", cmap='pink')\n",
        "    ax = plt.gca()\n",
        "    ax.grid(0)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.title(\"The grid\")\n",
        "    plt.text(2, 2, r\"$\\mathbf{S}$\", ha='center', va='center')\n",
        "    plt.text(8, 3, r\"$\\mathbf{G}$\", ha='center', va='center')\n",
        "    h, w = self._layout.shape\n",
        "    for y in range(h-1):\n",
        "      plt.plot([-0.5, w-0.5], [y+0.5, y+0.5], '-k', lw=2)\n",
        "    for x in range(w-1):\n",
        "      plt.plot([x+0.5, x+0.5], [-0.5, h-0.5], '-k', lw=2)\n",
        "\n",
        "\n",
        "  def get_obs(self):\n",
        "    y, x = self._state\n",
        "    return y*self._layout.shape[1] + x\n",
        "\n",
        "  def obs_to_state(obs):\n",
        "    x = obs % self._layout.shape[1]\n",
        "    y = obs // self._layout.shape[1]\n",
        "    s = np.copy(grid._layout)\n",
        "    s[y, x] = 4\n",
        "    return s\n",
        "\n",
        "  def step(self, action):\n",
        "    y, x = self._state\n",
        "\n",
        "    if action == 0:  # up\n",
        "      new_state = (y - 1, x)\n",
        "    elif action == 1:  # right\n",
        "      new_state = (y, x + 1)\n",
        "    elif action == 2:  # down\n",
        "      new_state = (y + 1, x)\n",
        "    elif action == 3:  # left\n",
        "      new_state = (y, x - 1)\n",
        "    else:\n",
        "      raise ValueError(\"Invalid action: {} is not 0, 1, 2, or 3.\".format(action))\n",
        "\n",
        "    new_y, new_x = new_state\n",
        "    if self._layout[new_y, new_x] == -1:  # wall\n",
        "      reward = -5.\n",
        "      discount = 0.9\n",
        "      new_state = (y, x)\n",
        "    elif self._layout[new_y, new_x] == 0:  # empty cell\n",
        "      reward = 0.\n",
        "      discount = 0.9\n",
        "    else:  # a goal\n",
        "      reward = self._layout[new_y, new_x]\n",
        "      discount = 0.\n",
        "      new_state = self._start_state\n",
        "    if self._noisy:\n",
        "      width = self._layout.shape[1]\n",
        "      reward += 2*np.random.normal(0, width - new_x + new_y)\n",
        "\n",
        "    self._state = new_state\n",
        "\n",
        "    return reward, discount, self.get_obs()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cOu9RZY3AkF1"
      },
      "cell_type": "markdown",
      "source": [
        "### Helper functions"
      ]
    },
    {
      "metadata": {
        "id": "6EttQGJ1n5Zn"
      },
      "cell_type": "code",
      "source": [
        "def run_experiment(env, agent, number_of_steps):\n",
        "    mean_reward = 0.\n",
        "    log_every = 100\n",
        "    mean_reward_log = np.zeros(number_of_steps//log_every-1)\n",
        "    try:\n",
        "      action = agent.initial_action()\n",
        "    except AttributeError:\n",
        "      action = 0\n",
        "    for i in range(number_of_steps):\n",
        "      reward, discount, next_state = grid.step(action)\n",
        "      action = agent.step(reward, discount, next_state)\n",
        "      mean_reward += (reward - mean_reward)/(i + 1.)\n",
        "      if i>0 and i % log_every == 0:\n",
        "        mean_reward_log[i//log_every-1] = mean_reward\n",
        "    return mean_reward, mean_reward_log\n",
        "\n",
        "\n",
        "def plot_values(values, colormap='pink', vmin=0, vmax=10):\n",
        "  plt.imshow(values, interpolation=\"nearest\", cmap=colormap, vmin=vmin, vmax=vmax)\n",
        "  plt.yticks([])\n",
        "  plt.xticks([])\n",
        "  plt.colorbar(ticks=[vmin, vmax])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fzpb_dGVjT0O"
      },
      "cell_type": "markdown",
      "source": [
        "# Part 1: Implement agents\n",
        "\n",
        "Each agent, should implement a step function:\n",
        "\n",
        "### `step(self, reward, discount, next_observation, ...)`:\n",
        "where `...` indicates there could be other inputs (discussed below).  The step should update the internal values, and return a new action to take.\n",
        "\n",
        "When the discount is zero ($\\text{discount} = \\gamma = 0$), then the `next_observation` will be the initial observation of the next episode.  One shouldn't bootstrap on the value of this state, which can simply be guaranteed when using \"$\\gamma \\cdot v(\\text{next_observation})$\" (for whatever definition of $v$ is appropriate) in the update, because $\\gamma = 0$.  So, the end of an episode can be seamlessly handled with the same step function.\n",
        "\n",
        "### `__init__(self, number_of_actions, number_of_states, initial_observation)`:\n",
        "The constructor will provide the agent the number of actions, number of states, and the initial observation. You can get the initial observation by first instatiating an environment, using `grid = Grid()`, and then calling `grid.get_obs()`.\n",
        "\n",
        "In this assignment, observations will be states in the environment, so the agent state, environment state, and observation will overlap, and we will use the word `state` interchangably with `observation`.\n",
        "\n",
        "All agents should be in pure Python (no need to use TensorFlow or PyTorch).  Using `numpy` is fine.\n",
        "\n",
        "### A note on the initial action\n",
        "Normally, you would also have to implement a method that gives the initial action, based on the initial state.  In our experiments the helper functions above will just use the action `0` (which corresponds to `up`) as initial action, so that otherwise we do not have to worry about this.  Note that this initial action is only executed once, and the beginning of the first episode---not at the beginning of each episode.\n",
        "\n",
        "Some algorithms (Q-learning, Sarsa) need to remember the last action in order to update its value when they see the next state.  In the `__init__`, make sure you set the initial action to zero, e.g.,\n",
        "```\n",
        "def __init__(...):\n",
        "  (...)\n",
        "  self._last_action = 0\n",
        "  (...)\n",
        "```\n"
      ]
    },
    {
      "metadata": {
        "id": "UaGeLcsvixmt"
      },
      "cell_type": "markdown",
      "source": [
        "### The grid\n",
        "\n",
        "The cell below shows the `Grid` environment that we will use. Here `S` indicates the start state and `G` indicates the goal.  The agent has four possible actions: up, right, down, and left.  Rewards are: `-5` for bumping into a wall, `+10` for reaching the goal, and `0` otherwise.  The episode ends when the agent reaches the goal, and otherwise continues.  The discount, on continuing steps, is $\\gamma = 0.9$.  Feel free to reference the implemetation of the `Grid` above, under the header \"a grid world\"."
      ]
    },
    {
      "metadata": {
        "id": "SlFuWFzIi5uB"
      },
      "cell_type": "code",
      "source": [
        "grid = Grid()\n",
        "grid.plot_grid()\n",
        "num_actions = 4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "j2N2gd11Qctt"
      },
      "cell_type": "markdown",
      "source": [
        "## Random agent"
      ]
    },
    {
      "metadata": {
        "id": "Vkbzl48jQcFn"
      },
      "cell_type": "code",
      "source": [
        "# For reference: here is a random agent\n",
        "class Random(object):\n",
        "\n",
        "  def __init__(self, number_of_actions, number_of_states, initial_state):\n",
        "    self._number_of_actions = number_of_actions\n",
        "\n",
        "  def step(self, reward, discount, next_state):\n",
        "    next_action = np.random.randint(number_of_actions)\n",
        "    return next_action"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "B8oKd0oyvNcH"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Agent 1: TD learning\n",
        "**[5 pts]** Implement an agent that behaves randomly, but that _on-policy_ estimates state values $v(s)$, using one-step TD learning with a step size $\\alpha=0.1$.\n",
        "\n",
        "Also implement `get_values(self)` that returns the vector of all state values (one value per state).\n",
        "\n",
        "You should be able to use the `__init__` as provided below, so you just have to implement `get_values` and `step`.  We store the initial state in the constructor because you need its value on the first `step` in order to compute the TD error when the first transition has occurred.  Hint: in the `step` you similarly will want to store the previous state to be able to compute the next TD error on the next step.\n"
      ]
    },
    {
      "metadata": {
        "id": "Hyo1QCD4kePY"
      },
      "cell_type": "code",
      "source": [
        "class RandomTD(object):\n",
        "\n",
        "  def __init__(self, number_of_states, number_of_actions, initial_state, step_size=0.1):\n",
        "    self._values = np.zeros(number_of_states)\n",
        "    self._state = initial_state\n",
        "    self._number_of_actions = number_of_actions\n",
        "    self._step_size = step_size\n",
        "\n",
        "  def get_values(self):\n",
        "    return self._values\n",
        "\n",
        "  def step(self, reward, discount, next_state):\n",
        "    \"\"\"TODO: Implement\"\"\"\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oaMmp1lDgpUG"
      },
      "cell_type": "markdown",
      "source": [
        "Run the next cell to run the `RandomTD` agent on a grid world."
      ]
    },
    {
      "metadata": {
        "id": "N0ZoYwgZfho2"
      },
      "cell_type": "code",
      "source": [
        "# DO NOT MODIFY THIS CELL\n",
        "grid = Grid()\n",
        "init_state = grid.get_obs()\n",
        "agent = RandomTD(grid._layout.size, num_actions, init_state)\n",
        "run_experiment(grid, agent, int(1e5))\n",
        "v = agent.get_values()\n",
        "plot_values(v.reshape(grid._layout.shape), colormap=\"hot\", vmin=-10, vmax=5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wxc_Sx7og4JH"
      },
      "cell_type": "markdown",
      "source": [
        "If everything worked as expected, the plot above will show the estimates state values under the random policy. This includes values for unreachable states --- on the walls and on the goal (we never actually reach the goal --- rather, the episode terminates on the transition to the goal.  The values on the walls and goal are, and will always remain, zero (shown in orange above).\n",
        "\n",
        "### Policy iteration\n",
        "We used TD to do policy evaluation for the random policy on this problem.  Consider doing policy improvement, by taking the greedy policy with respect to a one-step look-ahead.  For this, you may assume we have a true model - so for each state the policy would look at the value of the resulting state for each action, and would then pick the action with the highest state value. You do **not** have to implement this, just answer the following questions.\n",
        "\n",
        "**[5 pts]** Would the greedy step after one such iteration of policy evaluation and policy improvement be optimal on this problem?  Explain (in one or two sentences) why or why not.\n",
        "\n",
        "**[5 pts]** If we repeat the process over and over again, and repeatedly evaluate the greedy policy and then perform another improvement step, would then the policy eventually become optimal?  Explain (in one or two sentences) why or why not."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Agent 2: SARSA\n",
        "**[5 pts]** Implement an $\\epsilon$-greedy policy function.\n",
        "\n",
        "**[10 pts]** Implement a **Sarsa** agent. Now the `__init__` function receives a policy function as input, and you need to maintain a value function for (state, action) pairs, denoted by  **Q**.\n"
      ],
      "metadata": {
        "id": "4sovS6Vf0t2l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EpsilonGreedy(object):\n",
        "\n",
        "  def __init__(self, epsilon):\n",
        "    self._epsilon = epsilon\n",
        "\n",
        "  def get_action(self, Qvalues, state):\n",
        "    \"\"\"TODO: Implement\"\"\"\n",
        "    return action\n"
      ],
      "metadata": {
        "id": "m-nPf4ay8b55"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Sarsa(object):\n",
        "\n",
        "  def __init__(self, number_of_states, number_of_actions,\n",
        "               policy, initial_state, step_size=0.1):\n",
        "    self._Qvalues = np.zeros((number_of_states, number_of_actions))\n",
        "    self._state = initial_state\n",
        "    self._action = 0\n",
        "    self._number_of_actions = number_of_actions\n",
        "    self._policy = policy\n",
        "    self._step_size = step_size\n",
        "\n",
        "  def get_values(self):\n",
        "    return self._Qvalues\n",
        "\n",
        "  def step(self, reward, discount, next_state):\n",
        "    \"\"\"TODO: Implement\"\"\"\n",
        "    return next_action\n",
        "\n"
      ],
      "metadata": {
        "id": "VFUSLrZh1Lwt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run the next cell. This will train your Sarsa agent using an $\\epsilon$-greedy policy with $\\epsilon=0.1$.\n",
        "\n",
        "After training a state value is computed using a one-step greedy evaluation:\n",
        "$V(s) = \\max_a Q(s, a)$\n",
        "\n",
        "The figure on the left shows the value for each state in the grid, and the plot on the right shows the average reward received along the training episodes(always averaged from the first step)."
      ],
      "metadata": {
        "id": "JT0zl1GzrUih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DO NOT MODIFY THIS CELL\n",
        "# set-up epsilon greedy policy function with epsilon=0.1\n",
        "policy = EpsilonGreedy(epsilon=0.1)\n",
        "\n",
        "# start environment and get initial state\n",
        "grid = Grid()\n",
        "init_state = grid.get_obs()\n",
        "\n",
        "# start agent\n",
        "agent = Sarsa(grid._layout.size, num_actions, policy, init_state)\n",
        "\n",
        "# run expriments and get Q-values.\n",
        "mean_reward, mean_reward_log = run_experiment(grid, agent, int(1e5))\n",
        "q_values = agent.get_values()\n",
        "\n",
        "# compute the one-step greedy state values\n",
        "v = np.max(q_values, axis=1)\n",
        "\n",
        "# show results\n",
        "plt.figure(figsize=(20,5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plot_values(v.reshape(grid._layout.shape), colormap=\"hot\", vmin=-10, vmax=5)\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(mean_reward_log)\n",
        "t=plt.title('mean reward along training = {}'.format(mean_reward))\n"
      ],
      "metadata": {
        "id": "mJaDB3iL4ayh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Agent 3: Sarsa($\\lambda$)\n",
        "**[10 pts]** Implement a **Sarsa($\\lambda$)** agent.\n",
        "Now you need to maintain a table of eligibility traces, and update the Q-value of all (state, action) pairs in each step.\n"
      ],
      "metadata": {
        "id": "XpFDKf4hcktE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SarsaLambda(object):\n",
        "\n",
        "  def __init__(self, number_of_states, number_of_actions,\n",
        "               lmbda, policy, initial_state, step_size=0.1):\n",
        "    self._Qvalues = np.zeros((number_of_states, number_of_actions))\n",
        "    self._Etraces = np.zeros((number_of_states, number_of_actions))\n",
        "    self._state = initial_state\n",
        "    self._action = 0\n",
        "    self._number_of_actions = number_of_actions\n",
        "    self._lmbda = lmbda\n",
        "    self._policy = policy\n",
        "    self._step_size = step_size\n",
        "\n",
        "  def get_values(self):\n",
        "    return self._Qvalues\n",
        "\n",
        "  def step(self, reward, discount, next_state):\n",
        "    \"\"\"TODO: Implement\"\"\"\n",
        "    return next_action\n",
        "\n"
      ],
      "metadata": {
        "id": "qvuV_rwRXCLm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DO NOT MODIFY THIS CELL\n",
        "# set-up epsilon greedy policy function with epsilon=0.1\n",
        "policy = EpsilonGreedy(epsilon=0.1)\n",
        "\n",
        "lmbda = 0.1\n",
        "\n",
        "# start environment and get initial state\n",
        "grid = Grid()\n",
        "init_state = grid.get_obs()\n",
        "\n",
        "# start agent\n",
        "agent = SarsaLambda(\n",
        "    grid._layout.size, num_actions, lmbda, policy, grid.get_obs())\n",
        "\n",
        "# run expriments and get Q-values.\n",
        "mean_reward, mean_reward_log = run_experiment(grid, agent, int(1e5))\n",
        "q_values = agent.get_values()\n",
        "\n",
        "# compute the one-step greedy state values\n",
        "v = np.max(q_values, axis=1)\n",
        "\n",
        "# show results\n",
        "plt.figure(figsize=(20,5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plot_values(v.reshape(grid._layout.shape), colormap=\"hot\", vmin=-10, vmax=5)\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(mean_reward_log)\n",
        "t=plt.title('mean reward along training = {}'.format(mean_reward))\n"
      ],
      "metadata": {
        "id": "S-JVqUBub1d9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Agent 4: Q-learning\n",
        "**[10 pts]** Implement a **Q-learning** agent. The agent recevies a behavior policy at __init__, but always learns the Q values for the greedy policy.\n"
      ],
      "metadata": {
        "id": "6sMn3xx_DC2i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class QLearning(object):\n",
        "\n",
        "  def __init__(self, number_of_states, number_of_actions,\n",
        "               policy, initial_state, step_size=0.1):\n",
        "    self._Qvalues = np.zeros((number_of_states, number_of_actions))\n",
        "    self._state = initial_state\n",
        "    self._action = 0\n",
        "    self._number_of_actions = number_of_actions\n",
        "    self._policy = policy\n",
        "    self._step_size = step_size\n",
        "\n",
        "  def get_values(self):\n",
        "    return self._Qvalues\n",
        "\n",
        "  def step(self, reward, discount, next_state):\n",
        "    \"\"\"TODO: Implement\"\"\"\n",
        "    return next_action\n",
        "\n"
      ],
      "metadata": {
        "id": "Ju6_d-PQC04C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DO NOT MODIFY THIS CELL\n",
        "# set-up epsilon greedy policy function with epsilon=0.1\n",
        "policy = EpsilonGreedy(epsilon=0.1)\n",
        "\n",
        "# start environment and get initial state\n",
        "grid = Grid()\n",
        "init_state = grid.get_obs()\n",
        "\n",
        "# start agent\n",
        "agent = QLearning(grid._layout.size, num_actions, policy, grid.get_obs())\n",
        "\n",
        "# run expriments and get Q-values.\n",
        "mean_reward, mean_reward_log = run_experiment(grid, agent, int(1e5))\n",
        "q_values = agent.get_values()\n",
        "\n",
        "# compute the one-step greedy state values\n",
        "v = np.max(q_values, axis=1)\n",
        "\n",
        "# show results\n",
        "plt.figure(figsize=(20,5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plot_values(v.reshape(grid._layout.shape), colormap=\"hot\", vmin=-10, vmax=5)\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(mean_reward_log)\n",
        "t=plt.title('mean reward along training = {}'.format(mean_reward))\n"
      ],
      "metadata": {
        "id": "sdUL0p6cDbjE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Agent 5: General Q-learning\n",
        "**[15 pts]** Here you will consider a generalization of Q-learning and Sarsa.\n",
        "\n",
        "Remember from class, that Q learning can be implemented using any two policies:\n",
        "1. Behavior policy - used to act in the environment\n",
        "2. Target policy - used as a target for the value updates.\n",
        "\n",
        "Impement a **General Q-learning** agent, where the `__init__` takes as input a `target_policy` and a  `behavior_policy`.  The agent will act according to the behavior policy, and learn the Q-values of the target policy.\n",
        "The TD target becomes:\n",
        "$$r + \\gamma \\sum_a \\mathcal{P}_s^{\\texttt{target}}(a) Q(s, a))$$\n",
        "where $s$ is the new state sent as input to the agent's `step` function.\n",
        "\n",
        "The standard Q-learning is a special case where the target policy is the greedy policy, and Sarsa is the special case where the target policy is the same as the behavior policy.\n",
        "\n",
        "**[5 pts]** To implement this, you will first need to add a function to the `EpsilonGreedy` class that returns the probabilities of all actions for a given state."
      ],
      "metadata": {
        "id": "MuRL_Sfgfotj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EpsilonGreedy(object):\n",
        "\n",
        "  def __init__(self, epsilon):\n",
        "    self._epsilon = epsilon\n",
        "\n",
        "  def get_action_probs(self, Qvalues, state):\n",
        "    \"\"\"TODO: Implement\"\"\"\n",
        "    # Tip: given a vector v, you can use np.eye(len(v))[np.argmax(v)]\n",
        "    # to get a 'one-hot' vector which is zero everywhere except for a\n",
        "    # one where the first max of v is.\n",
        "\n",
        "    return action_probs\n",
        "\n",
        "  def get_action(self, Qvalues, state):\n",
        "    \"\"\"TODO: Implement\"\"\"\n",
        "    return action\n",
        "\n"
      ],
      "metadata": {
        "id": "2Us-B9iA-yVt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GeneralQLearning(object):\n",
        "\n",
        "  def __init__(self, number_of_states, number_of_actions,\n",
        "               behavior_policy, target_policy, initial_state, step_size=0.1):\n",
        "    self._Qvalues = np.zeros((number_of_states, number_of_actions))\n",
        "    self._state = initial_state\n",
        "    self._action = 0\n",
        "    self._number_of_actions = number_of_actions\n",
        "    self._behavior_policy = behavior_policy\n",
        "    self._target_policy = target_policy\n",
        "    self._step_size = step_size\n",
        "\n",
        "  def get_values(self):\n",
        "    return self._Qvalues\n",
        "\n",
        "  def step(self, reward, discount, next_state):\n",
        "    \"\"\"TODO: Implement\"\"\"\n",
        "    return next_action\n",
        "\n"
      ],
      "metadata": {
        "id": "MYVDHJiuk4sW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DO NOT MODIFY THIS CELL\n",
        "# set-up behavior and target policies\n",
        "b_policy = EpsilonGreedy(0.5)\n",
        "t_policy = EpsilonGreedy(0.)\n",
        "\n",
        "# start environment and get initial state\n",
        "grid = Grid()\n",
        "init_state = grid.get_obs()\n",
        "\n",
        "# start agent\n",
        "agent = GeneralQLearning(grid._layout.size, num_actions,\n",
        "                         b_policy, t_policy, grid.get_obs())\n",
        "\n",
        "# run expriments and get Q-values.\n",
        "mean_reward, mean_reward_log = run_experiment(grid, agent, int(1e5))\n",
        "q_values = agent.get_values()\n",
        "\n",
        "# compute the one-step greedy state values\n",
        "v = np.max(q_values, axis=1)\n",
        "\n",
        "# show results\n",
        "plt.figure(figsize=(20,5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plot_values(v.reshape(grid._layout.shape), colormap=\"hot\", vmin=-10, vmax=5)\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(mean_reward_log)\n",
        "t=plt.title('mean reward along training = {}'.format(mean_reward))\n"
      ],
      "metadata": {
        "id": "rvUYTFSG157j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1jZsPzCmDxAh"
      },
      "cell_type": "markdown",
      "source": [
        "# Part 2: Analyse Results"
      ]
    },
    {
      "metadata": {
        "id": "LGptHwE23lmP"
      },
      "cell_type": "markdown",
      "source": [
        "## Questions\n",
        "\n",
        "**[10 pts]** *How* do the policies found by Q-learning and Sarsa differ?  (Explain qualitatively how the behaviour differs in one or two sentences.)\n",
        "\n",
        "**[10 pts]** *Why* do the policies differ in this way?\n",
        "\n",
        "**[5 pts]** Run the Sarsa($\\lambda$) agent for different values of $\\lambda$ (e.g. 0.1 and 0.9), and report the results.\n",
        "\n",
        "**[5 pts]** Run the general Q-learning agent with different behavior policies and target policies, and report the results.\n",
        "\n",
        "For the last two questions you can generate plots, reports statistics of rewards and/or values. You can also state any qualitative observation that you make."
      ]
    }
  ]
}